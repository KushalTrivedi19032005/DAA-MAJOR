{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8c16cd50932453db6c01df65f6bb981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94d0e3d7193b4e1e98d3054bdb721d5b",
              "IPY_MODEL_24759e4eb1794cf5b56abcd8f632a3d1",
              "IPY_MODEL_b02e7ddfa6684e5d9c01c6d6c418ff19"
            ],
            "layout": "IPY_MODEL_f015fbbbd5f54cb78a90c728052059e0"
          }
        },
        "94d0e3d7193b4e1e98d3054bdb721d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ecfddef7b204865b7e494b2d710a90c",
            "placeholder": "​",
            "style": "IPY_MODEL_7464457ec3744c1c8dbe07d83327e8ab",
            "value": "model.safetensors: 100%"
          }
        },
        "24759e4eb1794cf5b56abcd8f632a3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9968da1faefd4616b834c97f5cffe9de",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8e0a98835b14de5a051ab57f42438ad",
            "value": 267954768
          }
        },
        "b02e7ddfa6684e5d9c01c6d6c418ff19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48b45c0b03841909b4c367042b48e3b",
            "placeholder": "​",
            "style": "IPY_MODEL_8a3c34f125174534935e9713ae0561ed",
            "value": " 268M/268M [00:08&lt;00:00, 33.1MB/s]"
          }
        },
        "f015fbbbd5f54cb78a90c728052059e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ecfddef7b204865b7e494b2d710a90c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7464457ec3744c1c8dbe07d83327e8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9968da1faefd4616b834c97f5cffe9de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e0a98835b14de5a051ab57f42438ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e48b45c0b03841909b4c367042b48e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3c34f125174534935e9713ae0561ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KushalTrivedi19032005/DAA-MAJOR/blob/main/IFNDipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose your GPU in runtime, run this cell, then restart runtime, and run all the following cells in order\n"
      ],
      "metadata": {
        "id": "Po7r6gU_TLU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "WekXWAQ-TFNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install focal_loss_torch"
      ],
      "metadata": {
        "id": "7Q83iygpCuSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration"
      ],
      "metadata": {
        "id": "TpixCPkgpIcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "CSV_PATH = \"/content/tokenized (1).csv\"\n",
        "IMAGE_ZIP_PATH = \"/content/resized_images.zip\"\n",
        "IMAGE_DIR = \"/content/sample_data/images\""
      ],
      "metadata": {
        "id": "kdY4TAhVpW-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTS**"
      ],
      "metadata": {
        "id": "R4RGkzKipx0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "# from transformers import DistilBertTokenizer\n",
        "from transformers.models.distilbert import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "EEsN7B-yp6E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA LOADING**"
      ],
      "metadata": {
        "id": "eNAst1Xap-Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_zip_path, image_dir, transform=None,\n",
        "                 tokenizer_name='distilbert-base-uncased', max_length=512):\n",
        "        self.csv_path = csv_path\n",
        "        self.image_zip_path = image_zip_path\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform if transform else transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Extract images from zip if not already extracted\n",
        "        if not os.path.exists(self.image_dir):\n",
        "            with zipfile.ZipFile(self.image_zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.image_dir)\n",
        "\n",
        "        # Load and clean the dataframe\n",
        "        self.df = pd.read_csv(self.csv_path, usecols=['id', 'Statement', 'Web', 'Category', 'Date', 'Label'])\n",
        "        self.df.dropna(subset=['Statement'], inplace=True)\n",
        "\n",
        "        # Use new scikit-learn API\n",
        "        self.web_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        self.cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "        # Fit and transform metadata\n",
        "        self.web_encoded = self.web_encoder.fit_transform(self.df[['Web']])\n",
        "        self.cat_encoded = self.cat_encoder.fit_transform(self.df[['Category']])\n",
        "\n",
        "# Assign unique column names to prevent index overlap\n",
        "        web_df = pd.DataFrame(self.web_encoded, columns=[f'web_{i}' for i in range(self.web_encoded.shape[1])])\n",
        "        cat_df = pd.DataFrame(self.cat_encoded, columns=[f'cat_{i}' for i in range(self.cat_encoded.shape[1])])\n",
        "\n",
        "# Concatenate metadata features\n",
        "        meta_df = pd.concat([web_df, cat_df], axis=1)\n",
        "\n",
        "# Convert to torch tensor\n",
        "        self.meta_features = torch.tensor(meta_df.values, dtype=torch.float32)\n",
        "\n",
        "        self.meta_dim = self.meta_features.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Text encoding\n",
        "        encoded = self.tokenizer(\n",
        "            row['Statement'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Image loading\n",
        "        img_path = os.path.join(self.image_dir, f\"img_{row['id']}.jpg\")\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "        except FileNotFoundError:\n",
        "            image = torch.zeros(3, 224, 224)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'image': image,\n",
        "            'label': int(row['Label']),\n",
        "            'id': int(row['id']),\n",
        "            'meta': self.meta_features[idx]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "nkvqOgHTqItk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEXT ENCODER**"
      ],
      "metadata": {
        "id": "2JdXCU-SqO5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "0kGaxWM5qcH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMAGE ENCODER**"
      ],
      "metadata": {
        "id": "fZNHJNmAqiY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x).squeeze(-1).squeeze(-1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "29EkzGxlquH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ATTENTION FUSION**"
      ],
      "metadata": {
        "id": "lrXPhYbVqxlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleAttentionFusion(nn.Module):\n",
        "    def __init__(self, text_dim=768, image_dim=2048, meta_dim=10, fusion_dim=512):\n",
        "        super(SimpleAttentionFusion, self).__init__()\n",
        "\n",
        "        # Project each modality to the same fusion dimension\n",
        "        self.text_proj = nn.Linear(text_dim, fusion_dim)\n",
        "        self.image_proj = nn.Linear(image_dim, fusion_dim)\n",
        "        self.meta_proj = nn.Linear(meta_dim, fusion_dim)\n",
        "\n",
        "        # Attention network to compute weights for each modality\n",
        "        self.attn = nn.Linear(fusion_dim, 1)\n",
        "\n",
        "    def forward(self, text_feat, image_feat, meta_feat):\n",
        "        # Project individual modality features to fusion dimension\n",
        "        text_feat = self.text_proj(text_feat)       # (batch_size, fusion_dim)\n",
        "        image_feat = self.image_proj(image_feat)    # (batch_size, fusion_dim)\n",
        "        meta_feat = self.meta_proj(meta_feat)       # (batch_size, fusion_dim)\n",
        "\n",
        "        # Stack modality features for attention\n",
        "        x = torch.stack([text_feat, image_feat, meta_feat], dim=1)  # (batch_size, 3, fusion_dim)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = torch.softmax(self.attn(x), dim=1)           # (batch_size, 3, 1)\n",
        "\n",
        "        # Apply attention weights to features\n",
        "        fused = torch.sum(attn_weights * x, dim=1)                  # (batch_size, fusion_dim)\n",
        "\n",
        "        return fused\n"
      ],
      "metadata": {
        "id": "lT5Qcbafq5HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP REDUCER**"
      ],
      "metadata": {
        "id": "DpoVDoLTrH9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPReducer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "PwcufOpXrM9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANFIS CLASSIFIER**"
      ],
      "metadata": {
        "id": "lcWh0MLGrRw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from itertools import product\n",
        "\n",
        "class ANFISClassifier(nn.Module):\n",
        "    def __init__(self, n_inputs, n_mfs):\n",
        "        super(ANFISClassifier, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_mfs = n_mfs\n",
        "        self.n_rules = n_mfs ** n_inputs\n",
        "\n",
        "        # Gaussian MF parameters: centers (c) and sigmas for each MF\n",
        "        self.c = nn.Parameter(torch.randn(n_inputs, n_mfs))       # (n_inputs, n_mfs)\n",
        "        self.sigma = nn.Parameter(torch.rand(n_inputs, n_mfs))    # (n_inputs, n_mfs)\n",
        "\n",
        "        # Consequent layer: one linear function per rule (n_inputs + 1 for bias)\n",
        "        self.rule_params = nn.Parameter(torch.randn(self.n_rules, n_inputs + 1))  # (n_rules, n_inputs+1)\n",
        "\n",
        "        # Precompute MF index combinations for all rules\n",
        "        self.rule_indices = self._compute_rule_indices()\n",
        "\n",
        "    def _compute_rule_indices(self):\n",
        "        # Cartesian product of MF indices for each input feature (n_mfs^n_inputs combinations)\n",
        "        return torch.tensor(list(product(range(self.n_mfs), repeat=self.n_inputs)), dtype=torch.long)\n",
        "\n",
        "    def gaussian_mf(self, x, c, sigma):\n",
        "        # Gaussian Membership Function\n",
        "        return torch.exp(-((x - c)**2) / (2 * sigma**2 + 1e-6))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Compute MF values: shape (batch, n_inputs, n_mfs)\n",
        "        mf_values = []\n",
        "        for i in range(self.n_inputs):\n",
        "            x_i = x[:, i].unsqueeze(1)                           # (batch, 1)\n",
        "            c_i = self.c[i].unsqueeze(0)                         # (1, n_mfs)\n",
        "            sigma_i = self.sigma[i].unsqueeze(0)                 # (1, n_mfs)\n",
        "            mf = self.gaussian_mf(x_i, c_i, sigma_i)             # (batch, n_mfs)\n",
        "            mf_values.append(mf)\n",
        "        mf_values = torch.stack(mf_values, dim=1)                # (batch, n_inputs, n_mfs)\n",
        "\n",
        "        # Compute firing strength of each rule: shape (batch, n_rules)\n",
        "        rule_strengths = []\n",
        "        for rule in self.rule_indices:\n",
        "            selected = mf_values[:, torch.arange(self.n_inputs), rule]  # (batch, n_inputs)\n",
        "            strength = torch.prod(selected, dim=1)                       # (batch,)\n",
        "            rule_strengths.append(strength)\n",
        "        w = torch.stack(rule_strengths, dim=1)                    # (batch, n_rules)\n",
        "\n",
        "        # Normalize firing strengths\n",
        "        normalized_w = w / (w.sum(dim=1, keepdim=True) + 1e-6)    # (batch, n_rules)\n",
        "\n",
        "        # Compute rule outputs: z_i = a1*x1 + a2*x2 + ... + an*xn + b\n",
        "        x_extended = torch.cat([x, torch.ones(batch_size, 1, device=x.device)], dim=1)  # (batch, n_inputs + 1)\n",
        "        z = torch.matmul(x_extended, self.rule_params.T)          # (batch, n_rules)\n",
        "\n",
        "        # Final output: weighted sum of rule outputs\n",
        "        output = (normalized_w * z).sum(dim=1)                    # (batch,)\n",
        "\n",
        "        return torch.sigmoid(output).unsqueeze(1)  # Now returns shape (batch, 1)\n",
        "         # Binary classification output\n"
      ],
      "metadata": {
        "id": "9rJol-OCrZQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FULL MODEL**"
      ],
      "metadata": {
        "id": "EAFsw8T6recK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FakeNewsDetectionModel(nn.Module):\n",
        "    def __init__(self, meta_dim):\n",
        "        super(FakeNewsDetectionModel, self).__init__()\n",
        "        self.text_encoder = TextEncoder()  # Outputs (batch_size, 768)\n",
        "        self.image_encoder = ResNetEncoder()  # Outputs (batch_size, 2048)\n",
        "\n",
        "        self.attn_fusion = SimpleAttentionFusion(\n",
        "            text_dim=768,\n",
        "            image_dim=2048,\n",
        "            meta_dim=meta_dim,\n",
        "            fusion_dim=512\n",
        "        )  # Outputs (batch_size, 512)\n",
        "\n",
        "        self.reducer = MLPReducer(input_dim=512, output_dim=4)  # Outputs (batch_size, 4)\n",
        "        self.anfis = ANFISClassifier(n_inputs=4, n_mfs=3)       # Outputs (batch_size,)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images, metadata, return_image_feats=False):\n",
        "        # Text features from transformer encoder\n",
        "        text_feats = self.text_encoder(input_ids, attention_mask)  # (batch, 768)\n",
        "\n",
        "        # Visual features from ResNet\n",
        "        image_feats = self.image_encoder(images)                   # (batch, 2048)\n",
        "\n",
        "        # Fused features using attention over text, image, metadata\n",
        "        fused_feats = self.attn_fusion(text_feats, image_feats, metadata)  # (batch, 512)\n",
        "\n",
        "        # Dimension reduction\n",
        "        reduced_feats = self.reducer(fused_feats)  # (batch, 4)\n",
        "\n",
        "        # ANFIS output\n",
        "        out = self.anfis(reduced_feats)  # (batch,)\n",
        "\n",
        "        if return_image_feats:\n",
        "            return out, image_feats\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0Xq11_zcrj_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CUSTOM LOSS**"
      ],
      "metadata": {
        "id": "rKyWjuVKrqPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, alpha_bce=0.5, beta_focal=2, gamma_huber=0.1, pos_weight=None, delta=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha_bce\n",
        "        self.beta = beta_focal\n",
        "        self.gamma = gamma_huber\n",
        "        self.delta = delta\n",
        "\n",
        "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        self.huber = nn.HuberLoss(delta=delta)\n",
        "\n",
        "    def focal_loss(self, probs, targets, gamma=2):\n",
        "        eps = 1e-8\n",
        "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
        "        focal_term = (1 - p_t) ** gamma\n",
        "        return -torch.mean(focal_term * torch.log(p_t + eps))\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        targets = targets.view(-1, 1).float()\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        focal_loss = self.focal_loss(probs, targets)\n",
        "        huber_loss = self.huber(probs, targets)\n",
        "        total_loss = self.alpha * bce_loss + self.beta * focal_loss + self.gamma * huber_loss\n",
        "        return total_loss, {\n",
        "            \"bce\": bce_loss.item(),\n",
        "            \"focal\": focal_loss.item(),\n",
        "            \"huber\": huber_loss.item()\n",
        "        }"
      ],
      "metadata": {
        "id": "LGqjJ1bFryBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tracking containers\n",
        "train_loss_history = {\"bce\": [], \"focal\": [], \"huber\": []}\n",
        "val_loss_history = {\"bce\": [], \"focal\": [], \"huber\": []}\n",
        "val_accuracy_history = []\n"
      ],
      "metadata": {
        "id": "-M6GryJXvokt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING and EVALUATION**"
      ],
      "metadata": {
        "id": "Y1XJIwOYr3Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize dataset and get meta_dim\n",
        "dataset = NewsDataset(\n",
        "    csv_path=CSV_PATH,\n",
        "    image_zip_path=IMAGE_ZIP_PATH,\n",
        "    image_dir=IMAGE_DIR\n",
        ")\n",
        "meta_dim = dataset.meta_dim\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = FakeNewsDetectionModel(meta_dim=meta_dim).to(device)\n",
        "pos_weight = torch.tensor([24.0], dtype=torch.float32).to(device)\n",
        "criterion = CustomLoss(pos_weight=pos_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Helper: Stratified split\n",
        "def stratified_split(dataset, test_size=0.2):\n",
        "    labels = [int(dataset.df.iloc[i]['Label']) for i in range(len(dataset))]\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        list(range(len(dataset))),\n",
        "        test_size=test_size,\n",
        "        stratify=labels,\n",
        "        random_state=None\n",
        "    )\n",
        "    return Subset(dataset, train_idx), Subset(dataset, test_idx)\n",
        "\n",
        "# Helper: Get DataLoaders\n",
        "def get_dataloader(dataset, batch_size, test_size=0.2):\n",
        "    train_set, test_set = stratified_split(dataset, test_size)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Helper: Evaluate model\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    loss_details = {\"bce\": 0.0, \"focal\": 0.0, \"huber\": 0.0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            images = batch['image'].to(device)\n",
        "            metadata = batch['meta'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs, _ = model(input_ids, attention_mask, images, metadata, return_image_feats=True)\n",
        "            loss, details = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            for key in loss_details:\n",
        "                loss_details[key] += details[key]\n",
        "\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct += (preds.squeeze().long() == labels.long()).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0.0\n",
        "    for key in loss_details:\n",
        "        loss_details[key] /= len(dataloader)\n",
        "    return total_loss, accuracy, loss_details\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loader, test_loader = get_dataloader(dataset, BATCH_SIZE)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    loss_details = {\"bce\": 0.0, \"focal\": 0.0, \"huber\": 0.0}\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        metadata = batch['meta'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(input_ids, attention_mask, images, metadata, return_image_feats=True)\n",
        "\n",
        "        loss, details = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        for key in loss_details:\n",
        "            loss_details[key] += details[key]\n",
        "\n",
        "    for key in loss_details:\n",
        "        train_loss_history[key].append(loss_details[key])\n",
        "        val_loss_history[key].append(val_details[key])\n",
        "\n",
        "    val_accuracy_history.append(val_acc)\n",
        "\n",
        "    val_loss, val_acc, val_details = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
        "print(f\"  Train Loss: {total_loss:.4f} | BCE: {loss_details['bce']:.4f}, Focal: {loss_details['focal']:.4f}, Huber: {loss_details['huber']:.4f}\")\n",
        "print(f\"  Val Loss:   {val_loss:.4f} | Acc: {val_acc:.4f} | BCE: {val_details['bce']:.4f}, Focal: {val_details['focal']:.4f}, Huber: {val_details['huber']:.4f}\")\n",
        "print(f\"  Val Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325,
          "referenced_widgets": [
            "f8c16cd50932453db6c01df65f6bb981",
            "94d0e3d7193b4e1e98d3054bdb721d5b",
            "24759e4eb1794cf5b56abcd8f632a3d1",
            "b02e7ddfa6684e5d9c01c6d6c418ff19",
            "f015fbbbd5f54cb78a90c728052059e0",
            "2ecfddef7b204865b7e494b2d710a90c",
            "7464457ec3744c1c8dbe07d83327e8ab",
            "9968da1faefd4616b834c97f5cffe9de",
            "d8e0a98835b14de5a051ab57f42438ad",
            "e48b45c0b03841909b4c367042b48e3b",
            "8a3c34f125174534935e9713ae0561ed"
          ]
        },
        "id": "88eu5PHPvgM_",
        "outputId": "73bf655e-8f77-43a3-8394-741cee172509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c16cd50932453db6c01df65f6bb981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 46.0MB/s]\n",
            "Epoch 1/2: 100%|██████████| 10640/10640 [31:05<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_details' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3014883974>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_details\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mval_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mval_accuracy_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_details' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses, accuracy, epochs):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    loss_names = [\"bce\", \"focal\", \"huber\"]\n",
        "    for i, loss_type in enumerate(loss_names):\n",
        "        axs[i].plot(range(1, epochs+1), train_losses[loss_type], label='Train')\n",
        "        axs[i].plot(range(1, epochs+1), val_losses[loss_type], label='Val')\n",
        "        axs[i].set_title(f\"{loss_type.upper()} Loss\")\n",
        "        axs[i].set_xlabel(\"Epoch\")\n",
        "        axs[i].set_ylabel(\"Loss\")\n",
        "        axs[i].legend()\n",
        "        axs[i].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy separately\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(range(1, epochs+1), accuracy, marker='o', color='green')\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "PiuQxvbnvbJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(train_loss_history, val_loss_history, val_accuracy_history, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Ux9bX6M5Ux_V",
        "outputId": "539770e6-f7c3-483c-80f9-e11a30d2bf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (2,) and (1,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2393833939>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-4192485244>\u001b[0m in \u001b[0;36mplot_losses\u001b[0;34m(train_losses, val_losses, accuracy, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bce\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"focal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huber\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{loss_type.upper()} Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    495\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (2,) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiva7GoApAeP",
        "outputId": "08deba57-92e9-438d-b3fa-3f41d0dc9359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1184828073>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/clean_EEG_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFQkIwtppGm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}